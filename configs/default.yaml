data:
  root: "./data"                # dataset folder (train/ and test/ inside)
  img_size: 224                 # input size (paper fixed at 224×224)
  grayscale_to_rgb: true        # convert 1-channel to 3-channel
  imagenet_norm: true           # normalize with ImageNet mean/std
  make_gray_flip_for_minority: true   # apply gray-flip augmentation
  minority_multiplier: 2        # how often minority classes can be sampled

train:
  batch_size: 32
  epochs: 30                   # start small (5), then 20–30 for paper replication
  lr: 0.001
  weight_decay: 0.0
  optimizer: "adamw"             # Adam optimizer (paper doesn’t specify) or AdamW for ViT
  pos_neg_ratio: 1.0            # 1:1 positive/negative sampling
  num_workers: 2
  save_dir: "./checkpoints"
  log_every: 100                # print loss every N steps

model:
  backbone: vit_b_16"  # paper’s chosen backbone
  embedding_dim: 10             # paper requirement (1×1×10 head)
  pretrained: true              # use ImageNet pretrained MobileNetV3

loss:
  type: "triplet"
  margin: 0.60                  # paper’s explicit margin

eval:
  test_pairs_csv: "./data/test_pairs.csv"  # generated by make_test_pairs.py
  device: "cuda"                 # will fall back to CPU if CUDA not available

seed: 42
